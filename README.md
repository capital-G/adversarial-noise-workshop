# Adversarial Noise Workshop

This workshop about Artificial Intelligence (AI) and its weakness to ‘Adversarial Attacks’ will tackle ways AI vision can be misled by adding visual noise to images, leading to malfunctions in the system. Javier will give an introduction to Adversarial Attacks applied to images and we will explore code examples that implement different adversarial techniques. This workshop will also propose a space for dialogue and discussion around the subject.

The project is subsidized by the Dutch funds *Stimuleringsfonds Creatieve Industrie* for Digital Culture.

The notes are from the workshop [*Adversarial Noise*](https://hsbxl.be/events/adversarial-noise/2021-09-26/) by Javier Lloret Pardo at HSBXL.

If you are unfamiliar with neural networks you can take a look at [teachable machine](https://teachablemachine.withgoogle.com/).

## How to work with this book

To work with the notebooks you can use [Binder](https://mybinder.org/), [Google Colab](https://colab.research.google.com) by clicking at the rocket at the of the page (not this one but on the following chapters it is shown).

You can also work locally on this, assuming you have a Unix based system and have git, pip3 and Python installed

```shell
# clone repo
git clone https://github.com/capital-G/adversarial-noise-workshop.git
cd adversarial-noise-workshop

# create venv and activate it
virtualenv venv
source venv/bin/activate

# install dependecies
pip3 install -r requirements.txt

# start jupyter lab
jupyter-lab
```

## License

The code is licensed with the MIT license.
