---
---

@misc{goodfellow2015explaining,
      title={Explaining and Harnessing Adversarial Examples}, 
      author={Ian J. Goodfellow and Jonathon Shlens and Christian Szegedy},
      year={2015},
      eprint={1412.6572},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@article{Su_2019,
   title={One Pixel Attack for Fooling Deep Neural Networks},
   volume={23},
   ISSN={1941-0026},
   url={http://dx.doi.org/10.1109/TEVC.2019.2890858},
   DOI={10.1109/tevc.2019.2890858},
   number={5},
   journal={IEEE Transactions on Evolutionary Computation},
   publisher={Institute of Electrical and Electronics Engineers (IEEE)},
   author={Su, Jiawei and Vargas, Danilo Vasconcellos and Sakurai, Kouichi},
   year={2019},
   month={Oct},
   pages={828–841}
}

@misc{brown2018adversarial,
      title={Adversarial Patch}, 
      author={Tom B. Brown and Dandelion Mané and Aurko Roy and Martín Abadi and Justin Gilmer},
      year={2018},
      eprint={1712.09665},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{wu2020making,
      title={Making an Invisibility Cloak: Real World Adversarial Attacks on Object Detectors}, 
      author={Zuxuan Wu and Ser-Nam Lim and Larry Davis and Tom Goldstein},
      year={2020},
      eprint={1910.14667},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{xu2020adversarial,
      title={Adversarial T-shirt! Evading Person Detectors in A Physical World}, 
      author={Kaidi Xu and Gaoyuan Zhang and Sijia Liu and Quanfu Fan and Mengshu Sun and Hongge Chen and Pin-Yu Chen and Yanzhi Wang and Xue Lin},
      year={2020},
      eprint={1910.11099},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{glasses,
author = {Sharif, Mahmood and Bhagavatula, Sruti and Bauer, Lujo and Reiter, Michael K.},
title = {Accessorize to a Crime: Real and Stealthy Attacks on State-of-the-Art Face Recognition},
year = {2016},
isbn = {9781450341394},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2976749.2978392},
doi = {10.1145/2976749.2978392},
abstract = {Machine learning is enabling a myriad innovations, including new algorithms for cancer
diagnosis and self-driving cars. The broad use of machine learning makes it important
to understand the extent to which machine-learning algorithms are subject to attack,
particularly when used in applications where physical security or safety is at risk.In
this paper, we focus on facial biometric systems, which are widely used in surveillance
and access control. We define and investigate a novel class of attacks: attacks that
are physically realizable and inconspicuous, and allow an attacker to evade recognition
or impersonate another individual. We develop a systematic method to automatically
generate such attacks, which are realized through printing a pair of eyeglass frames.
When worn by the attacker whose image is supplied to a state-of-the-art face-recognition
algorithm, the eyeglasses allow her to evade being recognized or to impersonate another
individual. Our investigation focuses on white-box face-recognition systems, but we
also demonstrate how similar techniques can be used in black-box scenarios, as well
as to avoid face detection.},
booktitle = {Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security},
pages = {1528–1540},
numpages = {13},
keywords = {adversarial machine learning, face recognition, face detection, neural networks},
location = {Vienna, Austria},
series = {CCS '16}
}
