
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>FGSM and BIM &#8212; Adversarial Noise</title>
    
  <link href="_static/css/theme.css" rel="stylesheet" />
  <link href="_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="One Pixel Attack" href="02-one-pixel-attack.html" />
    <link rel="prev" title="Adversarial Noise Workshop" href="README.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/noise.jpg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Adversarial Noise</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="README.html">
   Adversarial Noise Workshop
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   FGSM and BIM
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="02-one-pixel-attack.html">
   One Pixel Attack
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="03-adversarial-patch.html">
   Adversarial Patch Detection
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/01-FGSM.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/capital-G/adversarial-noise-workshop"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/capital-G/adversarial-noise-workshop/issues/new?title=Issue%20on%20page%20%2F01-FGSM.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/capital-G/adversarial-noise-workshop/main?urlpath=lab/tree/./01-FGSM.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/capital-G/adversarial-noise-workshop/blob/main/./01-FGSM.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#loading-the-model">
   Loading the model
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#loading-and-visualizing-images">
   Loading and visualizing images
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#using-the-model-to-predict-the-content-of-the-image">
   Using the model to predict the content of the image
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-fgsm-attack">
   The FGSM attack
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bibliography">
     Bibliography
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="fgsm-and-bim">
<span id="fgsm"></span><h1>FGSM and BIM<a class="headerlink" href="#fgsm-and-bim" title="Permalink to this headline">¶</a></h1>
<p>Although adversarial attacks can be used in different domains, in these workshop we will apply them to images, generating adversarial images that have the potential to mislead image classifiers.</p>
<p>In this notebook we will be using the Adversarial Attacks <em>FGSM</em> (Fast Gradient Sign Method) introduced in the publication <span id="id1">[<a class="reference internal" href="#id3">GSS15</a>]</span>.</p>
<p>This method is considered a <em>White-Box Attack</em>, meaning that we need to have access to the machine learning model to be able to perform the attack.</p>
<p>We will be working with the open source machine learning framework PyTorch for Python.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">List</span>
<span class="kn">import</span> <span class="nn">glob</span>
<span class="kn">import</span> <span class="nn">random</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">import</span> <span class="nn">torchvision</span>
<span class="kn">from</span> <span class="nn">torchvision.transforms</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># path to the data folder</span>
<span class="n">data_folder</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s1">&#39;data&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Before we can use the model to make any predictions, we need to select the pytorch device that we are going to use. The current versions of PyTorch supports GPU acceleration. This support has a major impact of the time needed, not only for training deep learning models but also for using them to make predictions</p>
<p>Although the current version of PyTorch (1.9, as August 2021) supports CUDA (for Nvidia’s GPUs) and ROCm (for AMD’s GPUs), the latest is still in a beta state.</p>
<p>As the computers in the cloud GPU service(s) has CUDA enabled Nvidia GPUs, we will use cuda to accelerate the computation</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># use compatible NVidia GPU if available</span>
<span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;cuda found!&#39;</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;no cuda found, will use cpu&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>no cuda found, will use cpu
</pre></div>
</div>
</div>
</div>
<div class="section" id="loading-the-model">
<h2>Loading the model<a class="headerlink" href="#loading-the-model" title="Permalink to this headline">¶</a></h2>
<p>To be able to use this attack, first we need to select the machine learning model that we will use as an image classifier. For that purpose, we we will use the models available in the library torchvision, that is part of pytorch.</p>
<p>These models have been pre-trained in 1000 categories from the image dataset <a class="reference external" href="https://image-net.org/">ImageNet</a>.</p>
<p>We can find the models available on the <a class="reference external" href="https://pytorch.org/vision/stable/index.html">torchvision website</a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># we set the parameters pretrained and progress to True, to download the pretrained model with a progress bar</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">alexnet</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">progress</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># when we load a model with pytorch, by default it is in train mode</span>
<span class="c1"># as we are going to use the model to make predictions we set it with evaluation mode</span>
<span class="c1"># with the ; we hide the output that was going to be printed out </span>
<span class="n">net</span><span class="o">.</span><span class="n">eval</span><span class="p">();</span> 
</pre></div>
</div>
</div>
</div>
<p>If a GPU is available we first need to pass the model to it. If it is not available the line will not change anything as it is already attached to the memomry of the CPU.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># we pass the model to the device just with this one line of code</span>
<span class="n">net</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">);</span>
</pre></div>
</div>
</div>
</div>
<p>We also need to load the labels associated to the categories used to train the model. In this case the categories are the 1000 used in the competition ImageNet Large Scale Visual Recognition Challenge.</p>
<p>In the text file synset_words.txt you can see all the categories of images that the mode has been trained on. These categories, also known as <i>synsets</i>, were inheret from project <a class="reference external" href="https://wordnet.princeton.edu/">Wordnet</a>.</p>
<p>They use several keywords to describe each category. In the file synset_words.txt, I abbreviated these keywords. You can find the original list in this link: <a class="reference external" href="https://gist.github.com/fnielsen/4a5c94eaa6dcdf29b7a62d886f540372">https://gist.github.com/fnielsen/4a5c94eaa6dcdf29b7a62d886f540372</a></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">data_folder</span><span class="p">,</span> <span class="s1">&#39;synset_words.txt&#39;</span><span class="p">),</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">synset_words</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="p">)[</span><span class="mi">1</span><span class="p">:])</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">f</span><span class="o">.</span><span class="n">readlines</span><span class="p">()]</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">synset_words</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([&#39;echidna, spiny anteater, anteater&#39;,
       &#39;bathtub, bathing tub, bath, tub&#39;,
       &#39;tobacco shop, tobacconist shop, tobacconist&#39;,
       &#39;white wolf, Arctic wolf, Canis lupus tundrarum&#39;, &#39;wombat&#39;],
      dtype=&#39;&lt;U121&#39;)
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="loading-and-visualizing-images">
<h2>Loading and visualizing images<a class="headerlink" href="#loading-and-visualizing-images" title="Permalink to this headline">¶</a></h2>
<p>We will load some images that we will use for our attacks with the library PIL.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">images</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">file_name</span> <span class="ow">in</span> <span class="n">glob</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">data_folder</span><span class="p">,</span> <span class="s1">&#39;images&#39;</span><span class="p">,</span> <span class="s1">&#39;dataset&#39;</span><span class="p">,</span> <span class="s1">&#39;*&#39;</span><span class="p">)):</span>
    <span class="c1"># remove file extension and path</span>
    <span class="n">short_file_name</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">splitext</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">file_name</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">images</span><span class="p">[</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">splitext</span><span class="p">(</span><span class="n">short_file_name</span><span class="p">)[</span><span class="mi">0</span><span class="p">]]</span> <span class="o">=</span> <span class="p">{</span>
        <span class="c1"># Image is from PIL library</span>
        <span class="s1">&#39;image&#39;</span><span class="p">:</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">file_name</span><span class="p">)</span>
    <span class="p">}</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;loaded </span><span class="si">{</span><span class="n">file_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>loaded data/images/dataset/dog.jpg
loaded data/images/dataset/harmonicawood.jpg
loaded data/images/dataset/pineapple.jpg
loaded data/images/dataset/joys.jpg
loaded data/images/dataset/jellyfish.jpg
loaded data/images/dataset/teapot.jpg
loaded data/images/dataset/pizza.jpg
loaded data/images/dataset/bus.jpg
loaded data/images/dataset/pig.jpg
loaded data/images/dataset/bear.jpg
</pre></div>
</div>
</div>
</div>
<p>Once we load the image, we are going to conduct a pre-processing step.
This pre-processing is necessary as the network was trained with this pre-processing, so if we want to use the net we need to take those steps into account as well.</p>
<p>We will also write a reverse function <code class="docutils literal notranslate"><span class="pre">plot_alexnet_image</span></code> which allows us to show us the image that was used in the network.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">IMAGENET_MEAN</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.485</span><span class="p">,</span> <span class="mf">0.456</span><span class="p">,</span> <span class="mf">0.406</span><span class="p">])</span>
<span class="n">IMAGENET_STD</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.229</span><span class="p">,</span> <span class="mf">0.224</span><span class="p">,</span> <span class="mf">0.225</span><span class="p">])</span>

<span class="c1"># classes are from torchvision.transform</span>
<span class="n">preprocess_alexnet</span> <span class="o">=</span> <span class="n">Compose</span><span class="p">([</span>
    <span class="n">Resize</span><span class="p">(</span><span class="mi">224</span><span class="p">),</span>
    <span class="n">CenterCrop</span><span class="p">(</span><span class="mi">224</span><span class="p">),</span>
    <span class="n">ToTensor</span><span class="p">(),</span>
    <span class="n">Normalize</span><span class="p">(</span><span class="n">IMAGENET_MEAN</span><span class="p">,</span> <span class="n">IMAGENET_STD</span><span class="p">)</span>
<span class="p">])</span>

<span class="k">def</span> <span class="nf">plot_alexnet_image</span><span class="p">(</span><span class="n">img_tensor</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="c1"># alexnet has this strange permutation of </span>
    <span class="n">img_tensor</span> <span class="o">=</span> <span class="n">img_tensor</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">img_np</span> <span class="o">=</span> <span class="n">img_tensor</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    
    <span class="c1"># _STD[None, None] adds 2 dimensions so we are multiplying the color dimension of the picture</span>
    <span class="n">img_np</span> <span class="o">=</span> <span class="p">(</span><span class="n">img_np</span> <span class="o">*</span> <span class="n">IMAGENET_STD</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span><span class="kc">None</span><span class="p">])</span> <span class="o">+</span> <span class="n">IMAGENET_MEAN</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span><span class="kc">None</span><span class="p">]</span>
    <span class="n">img_np</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">img_np</span><span class="p">,</span> <span class="n">a_min</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">a_max</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
    
    <span class="c1"># we plot the image </span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img_np</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Now we will choose a random image to start with.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># select one random image image</span>
<span class="n">random_file_name</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">images</span><span class="o">.</span><span class="n">keys</span><span class="p">()))</span>
<span class="n">random_image</span> <span class="o">=</span> <span class="n">images</span><span class="p">[</span><span class="n">random_file_name</span><span class="p">][</span><span class="s1">&#39;image&#39;</span><span class="p">]</span>

<span class="n">original_img_tensor</span> <span class="o">=</span> <span class="n">preprocess_alexnet</span><span class="p">(</span><span class="n">random_image</span><span class="p">)</span>
<span class="n">plot_alexnet_image</span><span class="p">(</span><span class="n">original_img_tensor</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Filename: </span><span class="si">{</span><span class="n">random_file_name</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Filename: bus
</pre></div>
</div>
<img alt="_images/01-FGSM_15_1.png" src="_images/01-FGSM_15_1.png" />
</div>
</div>
</div>
<div class="section" id="using-the-model-to-predict-the-content-of-the-image">
<h2>Using the model to predict the content of the image<a class="headerlink" href="#using-the-model-to-predict-the-content-of-the-image" title="Permalink to this headline">¶</a></h2>
<p>With the function <code class="docutils literal notranslate"><span class="pre">predict_image_top_categories</span></code> we use the model to predict the content in the image.
This function returns the number of categories, defined by the parameter <code class="docutils literal notranslate"><span class="pre">num_top_cat</span></code>, with a higher probability predicted by the model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">predict_image_top_categories</span><span class="p">(</span>
    <span class="n">img_tensor</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">models</span><span class="p">,</span>
    <span class="n">labels</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
    <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
    <span class="n">num_top_cat</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]:</span>
    <span class="c1"># create a mini-batch as expected by the model</span>
    <span class="c1"># add an extra batch dimension since pytorch treats all images as batches</span>
    <span class="n">input_batch</span> <span class="o">=</span> <span class="n">img_tensor</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    
    <span class="c1"># we send it to the device</span>
    <span class="n">input_batch</span> <span class="o">=</span> <span class="n">input_batch</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    
    <span class="c1"># forward pass, it returns unnormalized scores</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_batch</span><span class="p">)</span>

    <span class="c1"># we use the Softmax function to get the probability distribution over categories</span>
    <span class="n">probabilities</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
    
    <span class="c1"># show top categories per image</span>
    <span class="n">top_prob</span><span class="p">,</span> <span class="n">top_catid</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span><span class="n">probabilities</span><span class="p">,</span> <span class="n">num_top_cat</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">top_prob</span><span class="p">,</span> <span class="n">top_catid</span>
</pre></div>
</div>
</div>
</div>
<p>Now we can use this function to predict the content of our random image.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Prediction of image </span><span class="si">{</span><span class="n">random_file_name</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="n">confidences</span><span class="p">,</span> <span class="n">cat_ids</span> <span class="o">=</span> <span class="n">predict_image_top_categories</span><span class="p">(</span><span class="n">original_img_tensor</span><span class="p">,</span> <span class="n">net</span><span class="p">,</span> <span class="n">synset_words</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">num_top_cat</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="n">top_pred_id</span> <span class="o">=</span> <span class="n">cat_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="k">for</span> <span class="n">conf</span><span class="p">,</span> <span class="n">cat_id</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">confidences</span><span class="p">,</span> <span class="n">cat_ids</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Confidence </span><span class="si">{</span><span class="n">conf</span><span class="si">:</span><span class="s1">.2%</span><span class="si">}</span><span class="se">\t</span><span class="si">{</span><span class="n">cat_id</span><span class="si">}</span><span class="se">\t</span><span class="si">{</span><span class="n">synset_words</span><span class="p">[</span><span class="n">cat_id</span><span class="p">]</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Prediction of image bus
Confidence 40.86%	874	trolleybus, trolley coach, trackless trolley
Confidence 20.97%	867	trailer truck, tractor trailer, trucking rig, rig, articulated lorry, semi
Confidence 15.22%	654	minibus
Confidence 7.89%	705	passenger car, coach, carriage
Confidence 7.77%	555	fire engine, fire truck
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="the-fgsm-attack">
<h2>The FGSM attack<a class="headerlink" href="#the-fgsm-attack" title="Permalink to this headline">¶</a></h2>
<p>Now it’s time to define the function that will perform the FGSM adversarial attack. This function takes as an input the image (in a PyTorch tensor format), the model, the ID of the true category corresponding to the image, the device and a factor number (eps) that will determine the strength of the adversarial noise applied to the image.</p>
<p>Although by default this value was set to <span class="math notranslate nohighlight">\(0.007\)</span>, a higher value will often increase the chances to performa successful attack, but it till also make the adversarial noise more noticeable in the generated adversarial image.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">fgsm</span><span class="p">(</span>
    <span class="n">img_tensor</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">image_pred_label_idx</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
    <span class="n">eps</span><span class="o">=</span><span class="mf">0.007</span><span class="p">,</span> <span class="c1"># &quot;.007 corresponds to the magnitude of the smallest bit of an 8 bit image encoding after GoogLeNet’s conversion to real numbers.&quot;</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>  
    <span class="n">adv_noise</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">img_tensor</span><span class="p">)</span>
    
    <span class="n">img_tensor</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span> <span class="c1"># gradient required</span>

    <span class="c1"># create a mini-batch as expected by the model and send it to device</span>
    <span class="n">input_batch</span> <span class="o">=</span> <span class="n">img_tensor</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    
    <span class="n">model</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span> <span class="c1"># reset gradients</span>
    
    <span class="n">x</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_batch</span><span class="p">)</span> <span class="c1"># forward pass</span>
    
    <span class="c1"># define the loss function</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
    
    <span class="c1"># we create the label tensor and send it to device</span>
    <span class="n">label</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">image_pred_label_idx</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    
    <span class="c1"># calculate the loss</span>
    <span class="n">loss_cal</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
    <span class="c1"># perform a backward pass in order to get gradients</span>
    <span class="n">loss_cal</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    
    <span class="c1"># sign of data gradient of the loss func (with respect to input x)</span>
    <span class="c1"># as described in the paper.</span>
    <span class="n">data_grad_sign</span> <span class="o">=</span> <span class="n">img_tensor</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">sign</span><span class="p">()</span>
        
    <span class="c1"># for generating the adversarial image, we add the sign from the gradient multiplied by the epsilon</span>
    <span class="n">adv_noise</span> <span class="o">=</span> <span class="n">eps</span> <span class="o">*</span> <span class="n">data_grad_sign</span>
    <span class="n">adv_noise_full</span> <span class="o">=</span> <span class="n">data_grad_sign</span>
    
    <span class="c1"># and we merge it with the orginal image</span>
    <span class="n">adv_img_tensor</span> <span class="o">=</span> <span class="n">img_tensor</span> <span class="o">+</span> <span class="n">adv_noise</span>
    
    <span class="k">return</span> <span class="n">adv_img_tensor</span><span class="p">,</span> <span class="n">adv_noise</span><span class="p">,</span> <span class="n">adv_noise_full</span>
</pre></div>
</div>
</div>
</div>
<p>Now that we have defined the function, we can call it to conduct an adversarial attack with the loaded image. The function will return the adversarial image, and two images of the adversarial noise generated. The adversarial noise multiplied by the eps factor, and the adversarial noise in its full extent.</p>
<p>Now its time to perform the attack.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># adv_tensor_img, adv_tensor_noise, _ = fgsm(original_img_tensor, net, pred_idx, device, 0.13)</span>
<span class="n">adv_tensor_img</span><span class="p">,</span> <span class="n">adv_tensor_noise</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">fgsm</span><span class="p">(</span><span class="n">original_img_tensor</span><span class="p">,</span> <span class="n">net</span><span class="p">,</span> <span class="n">top_pred_id</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;adv image&quot;</span><span class="p">)</span>
<span class="n">plot_alexnet_image</span><span class="p">(</span><span class="n">adv_tensor_img</span><span class="p">)</span>
<span class="n">plot_alexnet_image</span><span class="p">(</span><span class="n">adv_tensor_noise</span><span class="o">+</span><span class="mf">0.0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>adv image
</pre></div>
</div>
<img alt="_images/01-FGSM_23_1.png" src="_images/01-FGSM_23_1.png" />
<img alt="_images/01-FGSM_23_2.png" src="_images/01-FGSM_23_2.png" />
</div>
</div>
<p>Now we can pass the adversarial image that we just generated to the model and see if it succeeds to mislead the model</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">confidences</span><span class="p">,</span> <span class="n">cat_ids</span> <span class="o">=</span> <span class="n">predict_image_top_categories</span><span class="p">(</span><span class="n">adv_tensor_img</span><span class="p">,</span> <span class="n">net</span><span class="p">,</span> <span class="n">synset_words</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">num_top_cat</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="k">for</span> <span class="n">conf</span><span class="p">,</span> <span class="n">cat_id</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">confidences</span><span class="p">,</span> <span class="n">cat_ids</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Confidence </span><span class="si">{</span><span class="n">conf</span><span class="si">:</span><span class="s1">.2%</span><span class="si">}</span><span class="se">\t</span><span class="si">{</span><span class="n">cat_id</span><span class="si">}</span><span class="se">\t</span><span class="si">{</span><span class="n">synset_words</span><span class="p">[</span><span class="n">cat_id</span><span class="p">]</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Confidence 86.89%	867	trailer truck, tractor trailer, trucking rig, rig, articulated lorry, semi
Confidence 3.26%	864	tow truck, tow car, wrecker
Confidence 2.39%	595	harvester, reaper
Confidence 2.15%	569	garbage truck, dustcart
Confidence 1.09%	555	fire engine, fire truck
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://pytorch.org/tutorials/beginner/fgsm_tutorial.html">Adversarial Example Generation in PyTorch</a></p></li>
<li><p><a class="reference external" href="https://savan77.github.io/blog/imagenet_adv_examples.html">Generating Adversarial Examples using PyTorch</a></p></li>
<li><p><a class="reference external" href="https://www.pyimagesearch.com/2021/03/01/adversarial-attacks-with-fgsm-fast-gradient-sign-method/">Adversarial attacks with FGSM (Fast Gradient Sign Method)</a></p></li>
<li><p><a class="reference external" href="https://www.pyimagesearch.com/2020/10/26/targeted-adversarial-attacks-with-keras-and-tensorflow/">Targeted adversarial attacks with Keras and TensorFlow</a></p></li>
<li><p><a class="reference external" href="https://towardsdatascience.com/perhaps-the-simplest-introduction-of-adversarial-examples-ever-c0839a759b8d">Perhaps the Simplest Introduction of Adversarial Examples Ever</a></p></li>
<li><p><a class="reference external" href="https://www.pyimagesearch.com/2020/10/26/targeted-adversarial-attacks-with-keras-and-tensorflow/">Targeted adversarial attacks with Keras and TensorFlow</a></p></li>
</ul>
<div class="section" id="bibliography">
<h3>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">¶</a></h3>
<p id="id2"><dl class="citation">
<dt class="label" id="id3"><span class="brackets"><a class="fn-backref" href="#id1">GSS15</a></span></dt>
<dd><p>Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. 2015. <a class="reference external" href="https://arxiv.org/abs/1412.6572">arXiv:1412.6572</a>.</p>
</dd>
</dl>
</p>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            



<div class='prev-next-bottom'>
    
    <div id="prev">
        <a class="left-prev" href="README.html" title="previous page">
            <i class="prevnext-label fas fa-angle-left"></i>
            <div class="prevnext-info">
                <p class="prevnext-label">previous</p>
                <p class="prevnext-title">Adversarial Noise Workshop</p>
            </div>
        </a>
    </div>
     <div id="next">
        <a class="right-next" href="02-one-pixel-attack.html" title="next page">
            <div class="prevnext-info">
                <p class="prevnext-label">next</p>
                <p class="prevnext-title">One Pixel Attack</p>
            </div>
            <i class="prevnext-label fas fa-angle-right"></i>
        </a>
     </div>

</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Javier Lloret Pardo<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>